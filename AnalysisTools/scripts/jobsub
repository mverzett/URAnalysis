#!/usr/bin/env python2
import sys, os, time
from pdb import set_trace
from glob import glob
from argparse import ArgumentParser
import re
import URAnalysis.Utilities.prettyjson as prettyjson
from URAnalysis.PlotTools.data_views import get_best_style, log #not for the same purpose, but the same mechanism
#import rootpy
log.setLevel(log.CRITICAL)

swdir = os.path.realpath(os.environ['URA_PROJECT'])
jobid = os.environ['jobid']
inputdir = os.path.join(swdir, 'inputs')
inputdir = os.path.join(inputdir, jobid)

parser = ArgumentParser('submit analyzer to the batch queues')
parser.add_argument('jobdir')
parser.add_argument('executable')
parser.add_argument('--opts', default='', help='options to be passed to the analyzer')
parser.add_argument('--samples', nargs='*', help='samples to be analyze (full regex supported')
parser.add_argument('--splitting', default='', help='samples to be analyze (full regex supported')

args = parser.parse_args()

jobdir = args.jobdir
exe = args.executable
jobargs = args.opts
filters = [re.compile(i) for i in args.samples] \
   if args.samples else [re.compile('.*')]

#os.mkdir('Production_'+ time.strftime("%Y-%m-%d_%H:%M:%S", time.gmtime()))
if os.path.isdir(jobdir):
	print jobdir, 'exists: EXIT'
	sys.exit(-1)
os.mkdir(jobdir)

samplefiles = glob(os.path.join(inputdir, '*.txt'))

#the cfg of the executable we want to run
exe_cfg = os.path.join(
        swdir,
        os.path.basename(exe).replace('.exe', '.cfg')
)

#external inputs: SFs, JEC and so forth
externals = glob(
        os.path.join(swdir, 'inputs', 'data', '*.*')
)

#external inputs, jobid specific: PU reweighting, TTSolver training (prob.root), ecc
externals_jobid_specific = glob(
        os.path.join(swdir, 'inputs', jobid, 'INPUT', '*.*')
)
transferfiles = [exe_cfg]+externals+externals_jobid_specific

transferfiles_config = ', '.join(transferfiles)

filesperjob = 10
splitting = None
if args.splitting:
  splitting = prettyjson.loads(open(args.splitting).read())

for sf in samplefiles:
  infile = os.path.join(inputdir, sf)
  sample = os.path.basename(sf).split('.txt')[0]
  if splitting:
    filesperjob = get_best_style(sample, splitting)
  if not any(i.match(sample) for i in filters): continue
  jobpath = os.path.join(
          jobdir, 
          sample
          )
  os.mkdir(jobpath)
  infiledes = open(infile, 'r')
  numrootfiles = infiledes.read().count('.root')
  infiledes.close()
  numjobs = max(numrootfiles/filesperjob, 1)
  numjobs = numjobs if numrootfiles > numjobs else numrootfiles
  print "submitting sample %s in %d jobs, ~1 every %d" % (sample, numjobs, filesperjob)

  condorfile ="""universe = vanilla
Executable = batch_job.sh
Should_Transfer_Files = YES
WhenToTransferOutput = ON_EXIT
Transfer_Input_Files = {5}
Output = con_$(Process).stdout
Error = con_$(Process).stderr
Arguments = {0} {1} {6}_out_$(Process).root -c {7} --thread 1 --j $(Process) --J {3} {4}
Queue {3}

	""".format(exe, infile, '', numjobs, jobargs, transferfiles_config, sample, os.path.basename(exe_cfg))
  
  conf = open(os.path.join(jobpath, 'condor.jdl'), 'w')
  conf.write(condorfile)
  conf.close()

  batch_job="""#!/bin/bash
WORKINGDIR=$PWD
cd {0}
source environment.sh
cd $WORKINGDIR

PA=$@
PA=${{PA#* }}

EXE=$1

pwd
ls -lht

echo $EXE
echo $PA

$EXE $PA 

exitcode=$? 
echo "exit code: "$exitcode
exit $exitcode 
	""".format(swdir)
  
  conf = open(os.path.join(jobpath, 'batch_job.sh'), 'w')
  conf.write(batch_job)
  conf.close()
  
  os.system('cd ' + jobpath + ' && condor_submit condor.jdl')

